% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/npdrLearner.R
\name{npdrLearnerCV}
\alias{npdrLearnerCV}
\title{\code{npdrLearnerCV}}
\usage{
npdrLearnerCV(
  x,
  label = "class",
  tune_grid = seq(10, 90, 10),
  dist_metric = "manhattan",
  tune_type = "knn",
  num_folds = 5,
  verbose = F
)
}
\arguments{
\item{x}{(m+1) x p dataframe of m instances, 1 class column and p attributes}

\item{label}{column label for class \code{"class"}}

\item{tune_grid}{vector of hyperparameter values to test for best 
classification accuracy}

\item{dist_metric}{for distance matrix between instances
(default: \code{"manhattan"}, others include \code{"euclidean"},
and for GWAS \code{"allele-sharing-manhattan"}).}

\item{tune_type}{type of hyperparmater to optimize. default: \code{"knn"}, 
others include \code{"ica"} (number of ica components for ica space 
transformation, and \code{"pca"} (number of components for PCA transformation.}

\item{num_folds}{number of cross-validation folds for tuning}
}
\value{
list containing best hyperparameter (best_param), its highest 
accuracy (best_acc), and a table of fold and parameter accuracies (cv_table)
}
\description{
Tune a hyperparmeter that maximizes the cross-validation accuracy of a k
-nearest-neighbors classifier. You can tune k, but keep in mind that the 
resulting k might be underestimated because the training sample size is
smaller than the original sample size. When other hyperparameters are 
optimized, k is fixed to the npdr theoretical value that adapts to the 
training size (todo: make more flexible with knn alpha). You can tune the 
number of ICA or PCA components as the components are used as the space for 
calculating nearest neighbors. todo: create function interface that allows 
user to create their own sapply_hyper_fn.
}
\examples{
library(flexclust) # need for npdrLearner knn classifier
library(fastICA)   # need if tuning ica tansformation
cv.out <- npdrLearnerCV(x=dats, label="class", 
              tune_grid = seq(20,90,5),   # tuning knn
              dist_metric = "manhattan",
              tune_type = "knn",
              num_folds=5, verbose=T)
cv.out$best_param
plot(cv.out$cv_table$hyp,cv.out$cv_table$means,
        xlab="hyperparameter", ylab="accuracy", 
        main="CV hyperparameter tuning", type="l")
text(cv.out$best_param,cv.out$best_acc,paste("max.loc =",cv.out$best_param))
Or you can tune number of knns 
cv.out <- npdrLearnerCV(x=dats, label="class", 
                     tune_grid = seq(20,90,5),   # tuning knn
                       dist_metric = "manhattan",
                       tune_type = "knn",
                       num_folds=5, verbose=T)
}
